# Redis Caching Implementation\n\nThis document describes the comprehensive Redis caching implementation for the trading platform.\n\n## Overview\n\nThe Redis caching layer provides high-performance data storage and retrieval for:\n- Market data (prices, order books, trades)\n- User sessions and preferences\n- Price alerts and monitoring\n- WebSocket connection state\n- Application performance optimization\n\n## Architecture\n\n### Components\n\n1. **Redis Client** (`redis_client.py`)\n   - Async Redis wrapper with connection pooling\n   - Error handling and reconnection logic\n   - JSON serialization support\n   - Comprehensive Redis operations (strings, hashes, lists, sets, sorted sets)\n\n2. **Base Cache** (`base_cache.py`)\n   - Abstract base class for specialized caches\n   - Key generation and prefix management\n   - TTL handling and expiration\n   - Batch operations support\n\n3. **Market Data Cache** (`market_data_cache.py`)\n   - Real-time price caching\n   - Order book storage\n   - Trade history management\n   - 24h statistics caching\n   - Symbol information storage\n\n4. **User Session Cache** (`user_session_cache.py`)\n   - Session lifecycle management\n   - User preferences storage\n   - WebSocket subscription tracking\n   - Rate limiting implementation\n\n5. **Price Cache** (`price_cache.py`)\n   - Time-series price data\n   - Price alert management\n   - OHLCV candlestick data\n   - Price change calculations\n\n6. **Cache Service** (`cache_service.py`)\n   - Centralized cache management\n   - Health monitoring\n   - Cleanup and maintenance\n   - Statistics and metrics\n\n7. **Cache Middleware** (`middleware.py`)\n   - HTTP request caching\n   - Response time optimization\n   - Cache headers management\n\n8. **Cached Repository** (`cached_repository.py`)\n   - Repository pattern integration\n   - Automatic cache invalidation\n   - Transparent caching layer\n\n## Configuration\n\n### Environment Variables\n\n```bash\n# Redis Connection\nREDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_DB=0\nREDIS_PASSWORD=\nREDIS_MAX_CONNECTIONS=20\n\n# Alternative: Redis URL\nREDIS_URL=redis://localhost:6379/0\n```\n\n### Settings Integration\n\n```python\n# settings.py\nclass Settings(BaseSettings):\n    REDIS_HOST: str = os.getenv(\"REDIS_HOST\", \"localhost\")\n    REDIS_PORT: int = int(os.getenv(\"REDIS_PORT\", \"6379\"))\n    REDIS_DB: int = int(os.getenv(\"REDIS_DB\", \"0\"))\n    REDIS_PASSWORD: str = os.getenv(\"REDIS_PASSWORD\", \"\")\n    REDIS_MAX_CONNECTIONS: int = 20\n```\n\n## Usage Examples\n\n### Basic Redis Operations\n\n```python\nfrom trading.infrastructure.cache import redis_client\n\n# Connect to Redis\nawait redis_client.connect()\n\n# Basic operations\nawait redis_client.set(\"key\", \"value\", ex=300)  # 5 minutes TTL\nvalue = await redis_client.get(\"key\")\nawait redis_client.delete(\"key\")\n\n# JSON data\ndata = {\"price\": 50000, \"volume\": 1000}\nawait redis_client.set(\"btc_data\", redis_client.get_json(data))\nretrieved = redis_client.parse_json(await redis_client.get(\"btc_data\"))\n\n# Hash operations\nawait redis_client.hset(\"user:123\", \"name\", \"John\")\nname = await redis_client.hget(\"user:123\", \"name\")\nuser_data = await redis_client.hgetall(\"user:123\")\n```\n\n### Market Data Caching\n\n```python\nfrom trading.infrastructure.cache import market_data_cache\n\n# Cache symbol price\nprice_data = {\n    \"symbol\": \"BTCUSDT\",\n    \"price\": 50000.00,\n    \"volume\": 1000.0,\n    \"change_24h\": 2.5\n}\n\nawait market_data_cache.set_symbol_price(\"BTCUSDT\", price_data, ttl=30)\nprice = await market_data_cache.get_symbol_price(\"BTCUSDT\")\n\n# Cache order book\norder_book = {\n    \"symbol\": \"BTCUSDT\",\n    \"bids\": [[49950.0, 1.5], [49940.0, 2.0]],\n    \"asks\": [[50050.0, 1.2], [50060.0, 0.8]]\n}\n\nawait market_data_cache.set_order_book(\"BTCUSDT\", order_book, ttl=10)\nbook = await market_data_cache.get_order_book(\"BTCUSDT\")\n\n# Add trade to history\ntrade_data = {\n    \"symbol\": \"BTCUSDT\",\n    \"price\": 50000.0,\n    \"quantity\": 0.1,\n    \"side\": \"buy\"\n}\n\nawait market_data_cache.add_trade_history(\"BTCUSDT\", trade_data)\ntrades = await market_data_cache.get_trade_history(\"BTCUSDT\", limit=10)\n\n# Get all cached symbols\nsymbols = await market_data_cache.get_all_symbols()\n\n# Check if price is fresh\nis_fresh = await market_data_cache.is_price_fresh(\"BTCUSDT\", max_age_seconds=60)\n```\n\n### User Session Management\n\n```python\nfrom trading.infrastructure.cache import user_session_cache\n\n# Create user session\nuser_id = \"user_123\"\nsession_data = {\n    \"device\": \"browser\",\n    \"ip_address\": \"192.168.1.100\",\n    \"user_agent\": \"Mozilla/5.0...\"\n}\n\nsession_id = await user_session_cache.set_user_session(user_id, session_data, ttl=3600)\n\n# Get session\nsession = await user_session_cache.get_user_session(user_id, session_id)\nsession_by_id = await user_session_cache.get_session_by_id(session_id)\n\n# Update activity\nawait user_session_cache.update_session_activity(\n    user_id, session_id, {\"last_action\": \"view_portfolio\"}\n)\n\n# Manage user preferences\npreferences = {\n    \"theme\": \"dark\",\n    \"language\": \"en\",\n    \"notifications\": True\n}\n\nawait user_session_cache.set_user_preferences(user_id, preferences)\nuser_prefs = await user_session_cache.get_user_preferences(user_id)\n\n# Rate limiting\ncount = await user_session_cache.set_rate_limit(user_id, \"api_call\", window=60)\nis_limited = await user_session_cache.is_rate_limited(user_id, \"api_call\", limit=100)\n\n# WebSocket subscriptions\nsubscription_data = {\"channels\": [\"orders\", \"risk_alerts\"]}\nawait user_session_cache.set_user_subscription(user_id, \"websocket\", subscription_data)\nsubscriptions = await user_session_cache.get_user_subscriptions(user_id)\n\n# Session cleanup\ndeleted_count = await user_session_cache.delete_all_user_sessions(user_id)\ncleanup_count = await user_session_cache.cleanup_expired_sessions()\n```\n\n### Price Monitoring and Alerts\n\n```python\nfrom trading.infrastructure.cache import price_cache\nfrom datetime import datetime, timedelta\n\n# Set current price\nawait price_cache.set_current_price(\"BTCUSDT\", 50000.0, volume=1000.0)\ncurrent_price = await price_cache.get_current_price(\"BTCUSDT\")\n\n# Get price changes\nchange_24h = await price_cache.get_price_change(\"BTCUSDT\", period_minutes=1440)\nchange_1h = await price_cache.get_price_change(\"BTCUSDT\", period_minutes=60)\n\n# Create price alert\nalert_id = await price_cache.set_price_alert(\n    user_id=\"user_123\",\n    symbol=\"BTCUSDT\",\n    target_price=55000.0,\n    condition=\"above\",\n    ttl=86400  # 24 hours\n)\n\n# Get user alerts\nuser_alerts = await price_cache.get_user_alerts(\"user_123\")\n\n# Check alerts (called when price updates)\ntriggered_alerts = await price_cache.check_price_alerts(\"BTCUSDT\", 55500.0)\n\n# Get price history\nhistory = await price_cache.get_price_history(\"BTCUSDT\", limit=100)\nstart_time = datetime.utcnow() - timedelta(hours=24)\nhistory_24h = await price_cache.get_price_history(\n    \"BTCUSDT\", start_time=start_time, limit=1000\n)\n\n# OHLCV data\nohlcv_1h = await price_cache.get_ohlcv_data(\"BTCUSDT\", \"1h\")\nohlcv_1d = await price_cache.get_ohlcv_data(\"BTCUSDT\", \"1d\")\n\n# Set OHLCV data\ncandles = [\n    {\"open\": 49000, \"high\": 51000, \"low\": 48000, \"close\": 50000, \"volume\": 1000},\n    # ... more candles\n]\nawait price_cache.set_ohlcv_data(\"BTCUSDT\", \"1h\", candles)\n```\n\n### Cache Service Management\n\n```python\nfrom trading.infrastructure.cache import cache_service\n\n# Service lifecycle\nawait cache_service.start()\nprint(f\"Cache running: {cache_service.is_running}\")\n\n# Health monitoring\nhealth = await cache_service.health_check()\nprint(f\"Status: {health['status']}\")\nprint(f\"Redis version: {health['redis_info']['version']}\")\n\n# Cache statistics\nsummary = await cache_service.get_cache_summary()\nprint(f\"Total keys: {summary['total_keys']}\")\nprint(f\"Cached symbols: {summary['symbol_count']}\")\n\n# Maintenance operations\ncleanup_results = await cache_service.cleanup_expired_data()\nprint(f\"Cleaned up: {cleanup_results}\")\n\n# Emergency cache clearing (use with caution)\nclear_results = await cache_service.clear_all_cache()\n\n# Get specific cache instances\nmarket_cache = cache_service.get_market_cache()\nsession_cache = cache_service.get_session_cache()\nprice_cache_instance = cache_service.get_price_cache()\n\n# Service shutdown\nawait cache_service.stop()\n```\n\n### Repository Caching Integration\n\n```python\nfrom trading.infrastructure.cache import cached_repository\nfrom trading.infrastructure.repositories.user_repository import UserRepository\n\n@cached_repository(cache_prefix=\"user\", default_ttl=600)\nclass CachedUserRepository(UserRepository):\n    pass\n\n# Usage\nuser_repo = CachedUserRepository()\n\n# These calls will be cached automatically\nuser = await user_repo.get_by_id(user_id, use_cache=True)  # Cache miss -> DB query -> Cache set\nuser_again = await user_repo.get_by_id(user_id, use_cache=True)  # Cache hit -> No DB query\n\n# Force refresh from database\nuser_fresh = await user_repo.get_by_id(user_id, use_cache=False)\n\n# Cache invalidation on write operations\nnew_user = await user_repo.create(user_entity)  # Automatically invalidates related cache\nupdated_user = await user_repo.update(user_entity)  # Automatically invalidates related cache\n\n# Manual cache invalidation\nawait user_repo.invalidate_cache(\"get_by_id*\")\n```\n\n### HTTP Response Caching\n\n```python\nfrom fastapi import APIRouter\nfrom trading.infrastructure.cache import cache_response\n\nrouter = APIRouter()\n\n@router.get(\"/market/{symbol}\")\n@cache_response(ttl=30, key_prefix=\"market_data\")\nasync def get_market_data(symbol: str, request: Request):\n    \"\"\"Get market data with 30-second caching.\"\"\"\n    # This response will be cached automatically\n    return {\n        \"symbol\": symbol,\n        \"price\": await get_current_price(symbol),\n        \"timestamp\": datetime.utcnow().isoformat()\n    }\n```\n\n## API Endpoints\n\n### Cache Management Endpoints\n\n```bash\n# Health check\nGET /api/v1/cache/health\n\n# Cache statistics\nGET /api/v1/cache/stats\n\n# Market data for symbol\nGET /api/v1/cache/market-data/{symbol}\n\n# Price data for symbol\nGET /api/v1/cache/price/{symbol}\n\n# All cached symbols\nGET /api/v1/cache/symbols\n\n# User sessions (authenticated)\nGET /api/v1/cache/sessions/{user_id}\n\n# Price alerts (authenticated)\nGET /api/v1/cache/price-alerts\nPOST /api/v1/cache/price-alert\nDELETE /api/v1/cache/price-alert/{alert_id}\n\n# Maintenance operations\nPOST /api/v1/cache/cleanup\nDELETE /api/v1/cache/clear?confirm=true\n```\n\n### Example API Responses\n\n**Health Check**\n```json\n{\n  \"status\": \"healthy\",\n  \"redis_connected\": true,\n  \"redis_info\": {\n    \"version\": \"7.0.8\",\n    \"used_memory\": \"2.5M\",\n    \"connected_clients\": 3,\n    \"uptime\": 86400\n  },\n  \"cache_stats\": {\n    \"market_data\": {\"total_keys\": 150},\n    \"user_session\": {\"total_keys\": 25},\n    \"price\": {\"total_keys\": 300}\n  },\n  \"is_running\": true\n}\n```\n\n**Market Data**\n```json\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"symbol\": \"BTCUSDT\",\n    \"price\": {\n      \"symbol\": \"BTCUSDT\",\n      \"price\": 50000.0,\n      \"volume\": 1000.0,\n      \"timestamp\": \"2023-12-01T12:00:00Z\"\n    },\n    \"order_book\": {\n      \"bids\": [[49950.0, 1.5]],\n      \"asks\": [[50050.0, 1.2]]\n    },\n    \"trade\": {\n      \"price\": 50000.0,\n      \"quantity\": 0.1,\n      \"side\": \"buy\"\n    }\n  }\n}\n```\n\n## Performance Optimization\n\n### Key Strategies\n\n1. **Prefix Organization**\n   - `market:price:BTCUSDT` - Current prices\n   - `market:orderbook:BTCUSDT` - Order books\n   - `session:user:123:abc-def` - User sessions\n   - `price:series:BTCUSDT` - Price time series\n\n2. **TTL Management**\n   - Real-time data: 10-30 seconds\n   - Market data: 1-5 minutes\n   - User sessions: 1 hour\n   - Static data: 1+ hours\n\n3. **Memory Optimization**\n   - JSON compression for large datasets\n   - Sorted sets for time series data\n   - Automatic cleanup of expired data\n   - Configurable key limits\n\n4. **Connection Pooling**\n   - Async connection pool\n   - Connection health monitoring\n   - Automatic reconnection\n   - Timeout handling\n\n### Performance Metrics\n\n```python\n# Monitor cache performance\nhealth = await cache_service.health_check()\nprint(f\"Memory usage: {health['redis_info']['used_memory']}\")\nprint(f\"Connected clients: {health['redis_info']['connected_clients']}\")\n\n# Cache hit rates (implement custom metrics)\nstats = await cache_service.get_cache_summary()\nprint(f\"Total cached keys: {stats['total_keys']}\")\nprint(f\"Cached symbols: {stats['symbol_count']}\")\n```\n\n## Testing\n\n### Unit Tests\n\n```bash\n# Run cache tests\npytest tests/integration/test_cache.py -v\n\n# Run with coverage\npytest tests/integration/test_cache.py --cov=trading.infrastructure.cache\n\n# Test specific components\npytest tests/integration/test_cache.py::TestRedisClient -v\npytest tests/integration/test_cache.py::TestMarketDataCache -v\n```\n\n### Integration Testing\n\n```python\n# Test complete cache flow\n@pytest.mark.asyncio\nasync def test_cache_integration():\n    await cache_service.start()\n    \n    # Test market data flow\n    await market_data_cache.set_symbol_price(\"TEST\", {\"price\": 100})\n    price = await market_data_cache.get_symbol_price(\"TEST\")\n    assert price[\"price\"] == 100\n    \n    # Test price alert flow\n    alert_id = await price_cache.set_price_alert(\"user\", \"TEST\", 110, \"above\")\n    triggered = await price_cache.check_price_alerts(\"TEST\", 115)\n    assert alert_id in triggered\n    \n    await cache_service.stop()\n```\n\n## Monitoring and Maintenance\n\n### Health Monitoring\n\n```python\n# Automated health checks\nasync def monitor_cache_health():\n    while True:\n        try:\n            health = await cache_service.health_check()\n            if health[\"status\"] != \"healthy\":\n                logger.error(f\"Cache unhealthy: {health}\")\n                # Send alert\n            \n            await asyncio.sleep(60)  # Check every minute\n        except Exception as e:\n            logger.error(f\"Health check failed: {e}\")\n            await asyncio.sleep(60)\n```\n\n### Maintenance Tasks\n\n```python\n# Scheduled maintenance\nasync def daily_cache_maintenance():\n    \"\"\"Run daily cache maintenance.\"\"\"\n    try:\n        # Clean up expired data\n        cleanup_results = await cache_service.cleanup_expired_data()\n        logger.info(f\"Daily cleanup: {cleanup_results}\")\n        \n        # Clean up old price data (older than 7 days)\n        price_cleanup = await price_cache.cleanup_old_data(days=7)\n        logger.info(f\"Price cleanup: {price_cleanup}\")\n        \n        # Clean up expired sessions\n        session_cleanup = await user_session_cache.cleanup_expired_sessions()\n        logger.info(f\"Session cleanup: {session_cleanup} sessions\")\n        \n    except Exception as e:\n        logger.error(f\"Maintenance failed: {e}\")\n\n# Schedule with asyncio or celery\nimport asyncio\nfrom apscheduler.schedulers.asyncio import AsyncIOScheduler\n\nscheduler = AsyncIOScheduler()\nscheduler.add_job(\n    daily_cache_maintenance,\n    'cron',\n    hour=2,  # Run at 2 AM daily\n    minute=0\n)\nscheduler.start()\n```\n\n### Memory Management\n\n```python\n# Monitor memory usage\nasync def check_memory_usage():\n    info = await redis_client.info()\n    used_memory = info.get('used_memory', 0)\n    max_memory = info.get('maxmemory', 0)\n    \n    if max_memory > 0:\n        usage_percent = (used_memory / max_memory) * 100\n        if usage_percent > 80:\n            logger.warning(f\"High memory usage: {usage_percent:.1f}%\")\n            # Trigger cleanup\n            await cache_service.cleanup_expired_data()\n```\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Connection Errors**\n   ```python\n   # Check Redis connectivity\n   try:\n       await redis_client.ping()\n       print(\"Redis connected\")\n   except Exception as e:\n       print(f\"Redis connection failed: {e}\")\n   ```\n\n2. **Memory Issues**\n   ```bash\n   # Check Redis memory usage\n   redis-cli info memory\n   \n   # Monitor key counts\n   redis-cli dbsize\n   \n   # Find large keys\n   redis-cli --bigkeys\n   ```\n\n3. **Performance Issues**\n   ```python\n   # Monitor slow queries\n   await redis_client.slowlog_get(10)\n   \n   # Check connection pool status\n   pool_info = redis_client._connection_pool\n   print(f\"Pool size: {pool_info.max_connections}\")\n   ```\n\n4. **Data Consistency**\n   ```python\n   # Verify cache data\n   cached_price = await market_data_cache.get_symbol_price(\"BTCUSDT\")\n   if cached_price:\n       age = datetime.utcnow() - datetime.fromisoformat(cached_price[\"timestamp\"])\n       if age.total_seconds() > 300:  # 5 minutes\n           logger.warning(f\"Stale cache data: {age.total_seconds()}s old\")\n   ```\n\n### Debugging Tools\n\n```python\n# Enable debug logging\nimport logging\nlogging.getLogger('trading.infrastructure.cache').setLevel(logging.DEBUG)\n\n# Monitor Redis commands\n# Add to redis_client.py\nclass DebugRedisClient(RedisClient):\n    async def get(self, key: str):\n        logger.debug(f\"REDIS GET {key}\")\n        result = await super().get(key)\n        logger.debug(f\"REDIS GET {key} -> {result is not None}\")\n        return result\n```\n\n## Security Considerations\n\n1. **Access Control**\n   - Use Redis AUTH if available\n   - Restrict network access to Redis\n   - Use SSL/TLS for connections\n\n2. **Data Protection**\n   - Encrypt sensitive data before caching\n   - Set appropriate TTLs for sensitive data\n   - Avoid caching PII without encryption\n\n3. **Input Validation**\n   - Validate cache keys\n   - Sanitize data before caching\n   - Implement rate limiting\n\n```python\n# Example: Encrypt sensitive data\nfrom cryptography.fernet import Fernet\n\nclass EncryptedCache(BaseCache):\n    def __init__(self, encryption_key: bytes):\n        super().__init__()\n        self.fernet = Fernet(encryption_key)\n    \n    def _serialize(self, value: Any) -> str:\n        json_str = super()._serialize(value)\n        encrypted = self.fernet.encrypt(json_str.encode())\n        return encrypted.decode()\n    \n    def _deserialize(self, value: str) -> Any:\n        decrypted = self.fernet.decrypt(value.encode())\n        return super()._deserialize(decrypted.decode())\n```\n\n## Deployment\n\n### Docker Configuration\n\n```dockerfile\n# Dockerfile\nFROM python:3.11-slim\n\n# Install Redis dependencies\nRUN pip install redis[hiredis] redis-om\n\n# Copy application\nCOPY . /app\nWORKDIR /app\n\n# Environment variables\nENV REDIS_HOST=redis\nENV REDIS_PORT=6379\nENV REDIS_DB=0\n\nCMD [\"uvicorn\", \"trading.app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\n```yaml\n# docker-compose.yml\nversion: '3.8'\n\nservices:\n  app:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - REDIS_HOST=redis\n    depends_on:\n      - redis\n\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis_data:/data\n    command: redis-server --appendonly yes\n\nvolumes:\n  redis_data:\n```\n\n### Production Considerations\n\n1. **High Availability**\n   - Redis Sentinel for failover\n   - Redis Cluster for scaling\n   - Connection pool configuration\n\n2. **Monitoring**\n   - Redis metrics collection\n   - Cache hit rate monitoring\n   - Memory usage alerts\n\n3. **Backup and Recovery**\n   - Redis persistence configuration\n   - Regular backup schedules\n   - Disaster recovery procedures\n\nThe Redis caching implementation provides a robust foundation for high-performance data access and real-time features in the trading platform!